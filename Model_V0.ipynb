{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model_V0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage.color import rgb2lab, rgb2gray, lab2rgb\n",
    "from skimage.io import imread, imshow, imsave\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, Conv2D, UpSampling2D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Create `ImageDataGenerators` for train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create image generator for image augmentation during training\n",
    "def TrainImage_a_b_gen(Xtrain, batch_size, validation_split):\n",
    "    '''Function that gets the training images using the flow method\n",
    "    via train_generator and applies the data augmentation of train_datagen.\n",
    "    This function is then used in the .fit() method when training the model.\n",
    "    \n",
    "    Note that this returns a tuple of (inputs, targets),\n",
    "    where inputs is a np.array with shape (batch_size, h, w, 1)\n",
    "    and targets is a np.array with shape (batch_sze, h, w, 2).'''\n",
    "    \n",
    "    train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                       shear_range = 0.4,\n",
    "                                       zoom_range = 0.4,\n",
    "                                       rotation_range = 45,\n",
    "                                       horizontal_flip = True,\n",
    "                                       validation_split = validation_split)\n",
    "    \n",
    "    train_generator = train_datagen.flow(x = Xtrain,\n",
    "                                         batch_size = batch_size,\n",
    "                                         subset = \"training\",\n",
    "                                         save_to_dir = \"Images/PokemonSilver_AugmentedTrain\", \n",
    "                                         save_prefix = \"PokemonSilver_Augmented\",\n",
    "                                         save_format = \"png\",\n",
    "                                         shuffle = True,\n",
    "                                         seed = 42)\n",
    "    \n",
    "    # Generate the tuple (X, Y) for each training batch\n",
    "    for batch in train_generator:\n",
    "        lab_batch = rgb2lab(batch)\n",
    "        X_batch = lab_batch[:, :, :, 0]\n",
    "        Y_batch = lab_batch[:, :, :, 1:]\n",
    "        Y_batch /= 128 # Normalise the values to [-1.0, 1.0]\n",
    "        yield (X_batch.reshape(X_batch.shape + (1, )), Y_batch)\n",
    "        \n",
    "\n",
    "def ValidationImage_a_b_gen(Xtrain, batch_size, validation_split):\n",
    "    \n",
    "    validation_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                            validation_split = validation_split)\n",
    "    \n",
    "    validation_generator = validation_datagen.flow(x = Xtrain,\n",
    "                                                   batch_size = batch_size,\n",
    "                                                   subset = \"validation\",\n",
    "                                                   save_to_dir = \"Images/PokemonSilver_Validation\", \n",
    "                                                   save_prefix = \"PokemonSilver_Validation\",\n",
    "                                                   save_format = \"png\",\n",
    "                                                   shuffle = True,\n",
    "                                                   seed = 42)\n",
    "    \n",
    "    # Generate the tuple (X, Y) for each validation batch\n",
    "    for batch in validation_generator:\n",
    "        lab_batch = rgb2lab(batch)\n",
    "        X_batch = lab_batch[:, :, :, 0]\n",
    "        Y_batch = lab_batch[:, :, :, 1:]\n",
    "        Y_batch /= 128\n",
    "        yield (X_batch.reshape(X_batch.shape + (1, )), Y_batch)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(name = \"Colouring-Pokemon\")\n",
    "\n",
    "model.add(InputLayer(input_shape = (288, 320, 1)))\n",
    "model.add(Conv2D(name = \"Conv2D_8a_3x3\", filters = 8, kernel_size = (3, 3), activation = 'relu', padding = 'same', strides = 2))\n",
    "model.add(Conv2D(name = \"Conv2D_8b_3x3\", filters = 8, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\n",
    "model.add(Conv2D(name = \"Conv2D_16a_3x3\", filters = 16, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\n",
    "model.add(Conv2D(name = \"Conv2D_16b_3x3\", filters = 16, kernel_size = (3, 3), activation = 'relu', padding = 'same', strides = 2))\n",
    "model.add(Conv2D(name = \"Conv2D_32a_3x3\", filters = 32, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\n",
    "model.add(Conv2D(name = \"Conv2D_32b_3x3\", filters = 32, kernel_size = (3, 3), activation = 'relu', padding = 'same', strides = 2))\n",
    "model.add(UpSampling2D(name = \"UpSamp-a_2x2\", size = (2, 2)))\n",
    "model.add(Conv2D(name = \"Conv2D_32c_3x3\", filters = 32, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\n",
    "model.add(UpSampling2D(name = \"UpSamp-b_2x2\", size = (2, 2)))\n",
    "model.add(Conv2D(name = \"Conv2D_16c_3x3\", filters = 16, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\n",
    "model.add(UpSampling2D(name = \"UpSamp-c_2x2\", size = (2, 2)))\n",
    "model.add(Conv2D(name = \"Conv2D_2_3x3\", filters = 2, kernel_size = (3, 3), activation = 'tanh', padding = 'same'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Colouring-Pokemon\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv2D_8a_3x3 (Conv2D)       (None, 144, 160, 8)       80        \n",
      "_________________________________________________________________\n",
      "Conv2D_8b_3x3 (Conv2D)       (None, 144, 160, 8)       584       \n",
      "_________________________________________________________________\n",
      "Conv2D_16a_3x3 (Conv2D)      (None, 144, 160, 16)      1168      \n",
      "_________________________________________________________________\n",
      "Conv2D_16b_3x3 (Conv2D)      (None, 72, 80, 16)        2320      \n",
      "_________________________________________________________________\n",
      "Conv2D_32a_3x3 (Conv2D)      (None, 72, 80, 32)        4640      \n",
      "_________________________________________________________________\n",
      "Conv2D_32b_3x3 (Conv2D)      (None, 36, 40, 32)        9248      \n",
      "_________________________________________________________________\n",
      "UpSamp-a_2x2 (UpSampling2D)  (None, 72, 80, 32)        0         \n",
      "_________________________________________________________________\n",
      "Conv2D_32c_3x3 (Conv2D)      (None, 72, 80, 32)        9248      \n",
      "_________________________________________________________________\n",
      "UpSamp-b_2x2 (UpSampling2D)  (None, 144, 160, 32)      0         \n",
      "_________________________________________________________________\n",
      "Conv2D_16c_3x3 (Conv2D)      (None, 144, 160, 16)      4624      \n",
      "_________________________________________________________________\n",
      "UpSamp-c_2x2 (UpSampling2D)  (None, 288, 320, 16)      0         \n",
      "_________________________________________________________________\n",
      "Conv2D_2_3x3 (Conv2D)        (None, 288, 320, 2)       290       \n",
      "=================================================================\n",
      "Total params: 32,202\n",
      "Trainable params: 32,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = 'rmsprop', loss = 'mse')\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Get train images from folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train images from folder\n",
    "Xtrain = []\n",
    "Xtrain_dir = Path(\"Images/PokemonSilver\")\n",
    "\n",
    "# Load image as numpy arrays to the list Xtrain\n",
    "for filename in os.listdir(Xtrain_dir):\n",
    "    if not filename.startswith('.'):\n",
    "        img_dir = Xtrain_dir / filename\n",
    "        Xtrain.append(img_to_array(load_img(img_dir)))\n",
    "\n",
    "# Convert the list of arrays into a 4D numpy array\n",
    "Xtrain = np.array(Xtrain, dtype = float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain shape: (6433, 288, 320, 3) <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Xtrain shape:\", Xtrain.shape, type(Xtrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train the model\n",
    "\n",
    "https://lambdalabs.com/blog/tensorflow-2-0-tutorial-03-saving-checkpoints/\n",
    "\n",
    "Training logs:\n",
    "\n",
    "**training_run01**\n",
    " - Size of training set = approx. 2000\n",
    " - Epochs = 10\n",
    " - Batch size = 20\n",
    " - Steps per epoch = 50\n",
    " - Validation steps = 1\n",
    " \n",
    "**training_run02**\n",
    " - Size of training set = approx. 6000\n",
    " - Epochs = 50\n",
    " - Batch size = 20\n",
    " - Steps per epoch = 289\n",
    " - Validation steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_model(training_run, Xtrain, model, validation_split, batch_size, epochs, steps_per_epoch, validation_steps, checkpoint_filepath):\n",
    "    \n",
    "    '''Function to train the model and save using ModelCheckpoint and EarlyStopping.'''\n",
    "    \n",
    "    print(\"=============================\")\n",
    "    print(f\" ##### TRAINING RUN {training_run} #####\")\n",
    "    print(\"=============================\")\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    # Print model summary\n",
    "    print(\"===============\")\n",
    "    print(\" MODEL SUMMARY \")\n",
    "    print(\"===============\")\n",
    "    print(model.summary())\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    # Print information about training and validation set\n",
    "    print(\"===========================\")\n",
    "    print(\" TRAIN/VAL DATASET SUMMARY \")\n",
    "    print(\"===========================\")\n",
    "    total_train_size = len(Xtrain)\n",
    "    train_size = round((1 - validation_split) * total_train_size)\n",
    "    validation_size = round(validation_split * total_train_size)\n",
    "    print(f\"Size of total training set    : {len(Xtrain)}\")\n",
    "    print(f\"(Training, validation) split  : ({1 - validation_split}, {validation_split})\")\n",
    "    print(f\"Size of training set          : {1 - validation_split} * {total_train_size} = {train_size}\")\n",
    "    print(f\"Size of validation set        : {validation_split} * {total_train_size} = {validation_size}\")\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    # Checkpoint\n",
    "    checkpoint = ModelCheckpoint(filepath = checkpoint_filepath,\n",
    "                                 monitor = \"val_loss\",\n",
    "                                 mode = \"min\",\n",
    "                                 save_best_only = True,\n",
    "                                 save_weights_only = False,\n",
    "                                 save_freq = \"epoch\",\n",
    "                                 verbose = 1)\n",
    "    \n",
    "    # Early Stop\n",
    "    earlystop = EarlyStopping(monitor = \"val_loss\",\n",
    "                              mode = \"min\",\n",
    "                              patience = 10,\n",
    "                              restore_best_weights = True,\n",
    "                              verbose = 1)\n",
    "    \n",
    "    callbacks_list = [checkpoint, earlystop]\n",
    "    \n",
    "    # Train model\n",
    "    print(\"=====================================\")\n",
    "    print(\" TRAIN, VALIDATE AND SAVE CHECKPOINT\")\n",
    "    print(\"=====================================\")\n",
    "    model_info = model.fit(x = TrainImage_a_b_gen(Xtrain, batch_size, validation_split),\n",
    "                           epochs = epochs,\n",
    "                           steps_per_epoch = steps_per_epoch,\n",
    "                           validation_data = ValidationImage_a_b_gen(Xtrain, batch_size, validation_split),\n",
    "                           validation_steps = validation_steps,\n",
    "                           callbacks = callbacks_list,\n",
    "                           verbose = 2)\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print(\"##### END OF TRAINING RUN. #####\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================\n",
      " ##### TRAINING RUN 02 #####\n",
      "=============================\n",
      "\n",
      "\n",
      "===============\n",
      " MODEL SUMMARY \n",
      "===============\n",
      "Model: \"Colouring-Pokemon\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv2D_8a_3x3 (Conv2D)       (None, 144, 160, 8)       80        \n",
      "_________________________________________________________________\n",
      "Conv2D_8b_3x3 (Conv2D)       (None, 144, 160, 8)       584       \n",
      "_________________________________________________________________\n",
      "Conv2D_16a_3x3 (Conv2D)      (None, 144, 160, 16)      1168      \n",
      "_________________________________________________________________\n",
      "Conv2D_16b_3x3 (Conv2D)      (None, 72, 80, 16)        2320      \n",
      "_________________________________________________________________\n",
      "Conv2D_32a_3x3 (Conv2D)      (None, 72, 80, 32)        4640      \n",
      "_________________________________________________________________\n",
      "Conv2D_32b_3x3 (Conv2D)      (None, 36, 40, 32)        9248      \n",
      "_________________________________________________________________\n",
      "UpSamp-a_2x2 (UpSampling2D)  (None, 72, 80, 32)        0         \n",
      "_________________________________________________________________\n",
      "Conv2D_32c_3x3 (Conv2D)      (None, 72, 80, 32)        9248      \n",
      "_________________________________________________________________\n",
      "UpSamp-b_2x2 (UpSampling2D)  (None, 144, 160, 32)      0         \n",
      "_________________________________________________________________\n",
      "Conv2D_16c_3x3 (Conv2D)      (None, 144, 160, 16)      4624      \n",
      "_________________________________________________________________\n",
      "UpSamp-c_2x2 (UpSampling2D)  (None, 288, 320, 16)      0         \n",
      "_________________________________________________________________\n",
      "Conv2D_2_3x3 (Conv2D)        (None, 288, 320, 2)       290       \n",
      "=================================================================\n",
      "Total params: 32,202\n",
      "Trainable params: 32,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "===========================\n",
      " TRAIN/VAL DATASET SUMMARY \n",
      "===========================\n",
      "Size of total training set    : 6433\n",
      "(Training, validation) split  : (0.9, 0.1)\n",
      "Size of training set          : 0.9 * 6433 = 5790\n",
      "Size of validation set        : 0.1 * 6433 = 643\n",
      "\n",
      "\n",
      "=====================================\n",
      " TRAIN, VALIDATE AND SAVE CHECKPOINT\n",
      "=====================================\n",
      "Epoch 1/50\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03494, saving model to SavedCheckpoints/trainingrun02/model.01-0.034935.hdf5\n",
      "1/1 - 4s - loss: 0.0075 - val_loss: 0.0349\n",
      "Epoch 2/50\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.03494 to 0.01845, saving model to SavedCheckpoints/trainingrun02/model.02-0.018447.hdf5\n",
      "1/1 - 1s - loss: 0.0321 - val_loss: 0.0184\n",
      "Epoch 3/50\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.01845\n",
      "1/1 - 1s - loss: 0.0148 - val_loss: 0.0275\n",
      "Epoch 4/50\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01845 to 0.00471, saving model to SavedCheckpoints/trainingrun02/model.04-0.004710.hdf5\n",
      "1/1 - 1s - loss: 0.0318 - val_loss: 0.0047\n",
      "Epoch 5/50\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00471\n",
      "1/1 - 1s - loss: 0.0275 - val_loss: 0.0134\n",
      "Epoch 6/50\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00471\n",
      "1/1 - 1s - loss: 0.0198 - val_loss: 0.0425\n",
      "Epoch 7/50\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00471\n",
      "1/1 - 1s - loss: 0.0303 - val_loss: 0.0242\n",
      "Epoch 8/50\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00471\n",
      "1/1 - 1s - loss: 0.0224 - val_loss: 0.0072\n",
      "Epoch 9/50\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00471\n",
      "1/1 - 1s - loss: 0.0212 - val_loss: 0.0376\n",
      "Epoch 10/50\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00471\n",
      "1/1 - 1s - loss: 0.0224 - val_loss: 0.0112\n",
      "Epoch 11/50\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00471\n",
      "1/1 - 1s - loss: 0.0200 - val_loss: 0.0253\n",
      "Epoch 12/50\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00471\n",
      "1/1 - 1s - loss: 0.0098 - val_loss: 0.0470\n",
      "Epoch 13/50\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00471\n",
      "1/1 - 1s - loss: 0.0095 - val_loss: 0.0163\n",
      "Epoch 14/50\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00471\n",
      "Restoring model weights from the end of the best epoch.\n",
      "1/1 - 1s - loss: 0.0225 - val_loss: 0.0317\n",
      "Epoch 00014: early stopping\n",
      "\n",
      "\n",
      "##### END OF TRAINING RUN. #####\n"
     ]
    }
   ],
   "source": [
    "train_and_save_model(training_run = \"02\", \n",
    "                     Xtrain = Xtrain,\n",
    "                     model = model,\n",
    "                     validation_split = 0.1,\n",
    "                     batch_size = 10,\n",
    "                     epochs = 50,\n",
    "                     steps_per_epoch = 1,\n",
    "                     validation_steps = 1,\n",
    "                     checkpoint_filepath = \"SavedCheckpoints/trainingrun02/model.{epoch:02d}-{val_loss:.6f}.hdf5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Get test images from folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "colourme = []\n",
    "colourme_dir = Path(\"Images/PokemonBlue\")\n",
    "\n",
    "# Load image as numpy arrays to the list X\n",
    "for filename in os.listdir(colourme_dir):\n",
    "    if not filename.startswith('.'):\n",
    "        img_dir = colourme_dir / filename\n",
    "        colourme.append(img_to_array(load_img(img_dir)))\n",
    "\n",
    "colourme = np.array(colourme, dtype = float)\n",
    "colourme = rgb2lab(1.0/255 * colourme)[:, :, :, 0]\n",
    "colourme = colourme.reshape(colourme.shape + (1, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colourme shape: (826, 288, 320, 1) <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"colourme shape:\", colourme.shape, type(colourme))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test the model (i.e. use for prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on colour_me\n",
    "output = model.predict(colourme)\n",
    "output *= 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Save colourised images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cleonwong/Desktop/Code/python-virtual-environments/Colouring-Pokemon_env/lib/python3.7/site-packages/skimage/color/colorconv.py:1128: UserWarning: Color data out of range: Z < 0 in 1 pixels\n",
      "  return xyz2rgb(lab2xyz(lab, illuminant, observer))\n",
      "/Users/cleonwong/Desktop/Code/python-virtual-environments/Colouring-Pokemon_env/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: Outputs_02/PokemonBlue_predicted49.png is a low contrast image\n",
      "  \n",
      "/Users/cleonwong/Desktop/Code/python-virtual-environments/Colouring-Pokemon_env/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: Outputs_02/PokemonBlue_predicted66.png is a low contrast image\n",
      "  \n",
      "/Users/cleonwong/Desktop/Code/python-virtual-environments/Colouring-Pokemon_env/lib/python3.7/site-packages/skimage/color/colorconv.py:1128: UserWarning: Color data out of range: Z < 0 in 2 pixels\n",
      "  return xyz2rgb(lab2xyz(lab, illuminant, observer))\n",
      "/Users/cleonwong/Desktop/Code/python-virtual-environments/Colouring-Pokemon_env/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: Outputs_02/PokemonBlue_predicted147.png is a low contrast image\n",
      "  \n",
      "/Users/cleonwong/Desktop/Code/python-virtual-environments/Colouring-Pokemon_env/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: Outputs_02/PokemonBlue_predicted148.png is a low contrast image\n",
      "  \n",
      "/Users/cleonwong/Desktop/Code/python-virtual-environments/Colouring-Pokemon_env/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: Outputs_02/PokemonBlue_predicted152.png is a low contrast image\n",
      "  \n",
      "/Users/cleonwong/Desktop/Code/python-virtual-environments/Colouring-Pokemon_env/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: Outputs_02/PokemonBlue_predicted169.png is a low contrast image\n",
      "  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-476bb590f4a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0moutput_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolourme\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moutput_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0moutput_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlab2rgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0moutput_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutput_array\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Convert to uint8 to avoid lossy conversion warning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mimsave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Outputs_02/PokemonBlue_predicted{i}.png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Code/python-virtual-environments/Colouring-Pokemon_env/lib/python3.7/site-packages/skimage/color/colorconv.py\u001b[0m in \u001b[0;36mlab2rgb\u001b[0;34m(lab, illuminant, observer)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0men\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwikipedia\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mwiki\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mStandard_illuminant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m     \"\"\"\n\u001b[0;32m-> 1128\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mxyz2rgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlab2xyz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0milluminant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobserver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Code/python-virtual-environments/Colouring-Pokemon_env/lib/python3.7/site-packages/skimage/color/colorconv.py\u001b[0m in \u001b[0;36mlab2xyz\u001b[0;34m(lab, illuminant, observer)\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.2068966\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m     \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m     \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m16.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m116.\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m7.787\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Output colourisations of colour_me\n",
    "for i in range(len(output)):\n",
    "    output_array = np.zeros((288, 320, 3))\n",
    "    output_array[:, :, 0] = colourme[i][:, :, 0]\n",
    "    output_array[:, :, 1:] = output[i]\n",
    "    output_array = lab2rgb(output_array)\n",
    "    output_array = (output_array * 255).astype(np.uint8) # Convert to uint8 to avoid lossy conversion warning\n",
    "    imsave(f\"Outputs_02/PokemonBlue_predicted{i}.png\", output_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
